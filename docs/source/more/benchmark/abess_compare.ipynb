{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with `abess`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abess is a well-known open-source library that supports variable selection. A comparative table summarizing the supported features of abess and skscope is listed below:\n",
    "\n",
    "| Supported features            | \\abess | \\skscope |\n",
    "|-------------------------------|--------|----------|\n",
    "| Sparsity matrices             | Covarites only | YES      |\n",
    "| Warm starts                   | YES    | YES      |\n",
    "| GPU/TPU                       | NO     | YES      |\n",
    "| Multi CPUs                    | YES    | YES      |\n",
    "| \\sklearn compatibility        | YES    | YES      |\n",
    "| General objective function    | YES    | YES      |\n",
    "| No. supported algorithms      | 1      | 7        |\n",
    "| Group-structure sparisty      | YES    | YES      |\n",
    "| Nuisance parameters           | YES    | YES      |\n",
    "\n",
    "*Table: The support features of the abess and skscope.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we compare the performance (recovery accuracy and computation time) of `skscope` and `abess`on three tasks as follows:\n",
    "\n",
    "- Linear Regression\n",
    "- Non-negative Linear Regression\n",
    "- Logistic Regression.\n",
    "\n",
    "Besides, we compare the corresponding performance with different data (sample size $n$, feature number $p$ and support size $s$) dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from skscope import ScopeSolver\n",
    "\n",
    "\n",
    "from abess.datasets import make_glm_data\n",
    "from abess import LinearRegression, LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the main test function which records the recovery accuracy and computation time of both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_time(n=500, p=1000, s=5, random_state=None, rep=1):\n",
    "    print('='*20 + f'  n={n}, p={p}, s={s}  ' + '='*20 )\n",
    "    # rng = np.random.default_rng(random_state)\n",
    "    # true_support_set = rng.choice(np.arange(p), size=s, replace=False)\n",
    "    # real_coef = np.zeros(p)\n",
    "\n",
    "    # iterables = [['Linear', 'Non-Neg', 'Logistic'], ['abess', 'skscope']]\n",
    "    # index = pd.MultiIndex.from_product(iterables, names=['Task', 'Model'])\n",
    "    # res = pd.DataFrame(columns=['Accuracy', 'Time'], index = index)\n",
    "    res = pd.DataFrame(columns=['Task', 'Model', 'Accuracy', 'Time'])\n",
    "\n",
    "    for i in range(rep):\n",
    "        rng = np.random.default_rng(i)\n",
    "        true_support_set = rng.choice(np.arange(p), size=s, replace=False)\n",
    "        real_coef = np.zeros(p)\n",
    "        for task in ['Linear', 'Non-Neg', 'Logistic']:\n",
    "            if task == 'Linear':\n",
    "                real_coef[true_support_set] = rng.choice(np.arange(1, 4), size=s) * rng.choice([1, -1], size=s)\n",
    "                data = make_glm_data(n=n, p=p, k=s, family='gaussian', coef_=real_coef)\n",
    "                model = LinearRegression(support_size=s)\n",
    "                def objective(params):\n",
    "                    loss = jnp.mean((y - X @ params) ** 2)\n",
    "                    return loss\n",
    "                solver = ScopeSolver(p, sparsity=s)\n",
    "            elif task == 'Non-Neg':\n",
    "                real_coef[true_support_set] = rng.choice(np.arange(1, 4), size=s) * rng.choice([1], size=s)\n",
    "                data = make_glm_data(n=n, p=p, k=s, family='gaussian', coef_=real_coef)\n",
    "                model = LinearRegression(support_size=s)\n",
    "                def objective(params):\n",
    "                    loss = jnp.mean((y - X @ params) ** 2)\n",
    "                    return loss\n",
    "                solver = ScopeSolver(p, sparsity=s)\n",
    "            elif task == 'Logistic':\n",
    "                real_coef[true_support_set] = rng.choice(np.arange(1, 4), size=s) * rng.choice([1, -1], size=s)\n",
    "                data = make_glm_data(n=n, p=p, k=s, family='binomial', coef_=real_coef)\n",
    "                model = LogisticRegression(support_size=s)\n",
    "                def objective(params):\n",
    "                    xbeta = jnp.clip(X @ params, -30, 30)\n",
    "                    return jnp.mean(jnp.log(1 + jnp.exp(xbeta)) - y * xbeta)\n",
    "                solver = ScopeSolver(p, sparsity=s)\n",
    "        \n",
    "            X, y = data.x, data.y\n",
    "            \n",
    "            # abess\n",
    "            t_begin = time.time()\n",
    "            model.fit(X, y)\n",
    "            t_abess = time.time() - t_begin\n",
    "            acc_abess = len(set(np.nonzero(model.coef_)[0]) & set(true_support_set)) / s\n",
    "            # res.loc[(type, 'abess')] = [acc_abess, np.round(t_abess, 4)]\n",
    "            res.loc[len(res)] = [task, 'abess', acc_abess, t_abess]\n",
    "            \n",
    "            # skscope\n",
    "            t_begin = time.time()\n",
    "            params = solver.solve(objective, jit=True)\n",
    "            t_skscope = time.time() - t_begin\n",
    "            acc_skscope = len(set(np.nonzero(params)[0]) & set(np.nonzero(data.coef_)[0])) / s\n",
    "            # res.loc[(type, 'skscope')] = [acc_skscope, np.round(t_skscope, 4)]\n",
    "            res.loc[len(res)] = [task, 'skscope', acc_skscope, t_skscope]\n",
    "\n",
    "    res_mean = res.groupby(['Task', 'Model']).mean()\n",
    "    res_std = res.groupby(['Task', 'Model']).std()\n",
    "    res_mean['Accuracy'] = res_mean['Accuracy'].map(lambda x: f'{x:.2f}') \n",
    "    res_mean['Time'] = res_mean['Time'].map(lambda x: f'{x:.2f}') \n",
    "    res_std['Accuracy'] = res_std['Accuracy'].map(lambda x: f' ({x:.2f})') \n",
    "    res_std['Time'] = res_std['Time'].map(lambda x: f' ({x:.2f})') \n",
    "    res_all = res_mean + res_std\n",
    "    print(res_all)\n",
    "    return res_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare these three tasks with $$(n, p, s)\\in\\{(500, 1000, 5), (2500, 5000, 25), (5000, 10000, 50)\\}.$$\n",
    "\n",
    "Certainly, `abess` is faster than `skscope` since all three tasks are generalized linear model and are specicialized class for `abess`.\n",
    "\n",
    "This phenomenon is admmissible since the auto-differention procedure of `skscope` is general and takes more computation time.\n",
    "\n",
    "All results are shwon in the following tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================  n=500, p=1000, s=10  ====================\n",
      "                     Accuracy         Time\n",
      "Task     Model                            \n",
      "Linear   abess    1.00 (0.00)  0.01 (0.00)\n",
      "         skscope  1.00 (0.00)  0.23 (0.03)\n",
      "Logistic abess    0.99 (0.03)  0.02 (0.00)\n",
      "         skscope  0.99 (0.03)  0.26 (0.03)\n",
      "Non-Neg  abess    1.00 (0.00)  0.01 (0.00)\n",
      "         skscope  1.00 (0.00)  0.22 (0.02)\n",
      "====================  n=2500, p=5000, s=50  ====================\n",
      "                     Accuracy         Time\n",
      "Task     Model                            \n",
      "Linear   abess    1.00 (0.00)  0.55 (0.12)\n",
      "         skscope  1.00 (0.00)  3.88 (1.10)\n",
      "Logistic abess    1.00 (0.00)  0.83 (0.15)\n",
      "         skscope  1.00 (0.00)  5.00 (0.84)\n",
      "Non-Neg  abess    1.00 (0.00)  0.63 (0.27)\n",
      "         skscope  1.00 (0.00)  3.86 (1.01)\n",
      "====================  n=5000, p=10000, s=100  ====================\n",
      "                     Accuracy          Time\n",
      "Task     Model                             \n",
      "Linear   abess    1.00 (0.00)   2.19 (0.22)\n",
      "         skscope  1.00 (0.00)  14.09 (1.28)\n",
      "Logistic abess    1.00 (0.00)   6.81 (3.32)\n",
      "         skscope  1.00 (0.00)  24.12 (5.83)\n",
      "Non-Neg  abess    1.00 (0.00)   2.26 (0.10)\n",
      "         skscope  1.00 (0.00)  14.80 (1.00)\n"
     ]
    }
   ],
   "source": [
    "settings = [\n",
    "    (500, 1000, 10),\n",
    "    (2500, 5000, 50),\n",
    "    (5000, 10000, 100),\n",
    "]\n",
    "for setting in settings:\n",
    "    n, p, s = setting\n",
    "    res = test_time(n=n, p=p, s=s, rep=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skscope",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
