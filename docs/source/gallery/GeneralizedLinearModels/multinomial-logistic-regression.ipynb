{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2a702d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63a7bf3",
   "metadata": {},
   "source": [
    "## Multinomial logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44388535",
   "metadata": {},
   "source": [
    "We would like to use an example to show how the sparse-constrained optimization for multinomial logistic regression works in our program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d013f51",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Multinomial logistic regression, also known as multinomial logit, is a classification method that generalizes logistic regression to multiclass problems. While binary logistic regression deals with two classes, multinomial logistic regression can handle three or more classes. This makes it particularly useful in scenarios where the outcome variable can take on more than two discrete values.\n",
    "\n",
    "The multinomial logistic regression model estimates the probability of each possible outcome of a categorical dependent variable, given a set of independent variables. The model is based on the logistic function, which ensures that the estimated probabilities are between 0 and 1 and that they sum to 1 across all categories.\n",
    "Next are some applications of it.\n",
    "\n",
    "- **Marketing and customer segmentation**: By identifying distinct customer segments, companies can tailor their marketing strategies and improve targeting, ultimately increasing sales and customer satisfaction.\n",
    "- **Medical research and diagnostics**: In medical research, multinomial logistic regression can help diagnose diseases based on patient symptoms and test results. This aids in early detection and personalized treatment plans, improving patient outcomes.\n",
    "- **Social sciences**: In social sciences, multinomial logistic regression is employed to study voting behavior, social mobility, and public opinion. This provides valuable insights into societal trends and helps inform policy decisions.\n",
    "\n",
    "\n",
    "In multinomial logistic regression, we have multiple categories, denoted by $k=1,2,...,K$. We want to predict the probability of each category given a set of predictor variables $X$. We assume that the probability of each category is a function of the predictor variables, and that the probabilities for each category sum to 1.\n",
    "\n",
    "We can model the probability of each category using the softmax function:\n",
    "\n",
    "$$P(Y=k|X=x) = \\frac{e^{\\beta _{0k} + \\beta _k^TX}}{\\sum_{j=1}^K e^{\\beta _{0j} + \\beta _j^TX}}$$\n",
    "\n",
    "where $Y$ is the categorical outcome, $X$ is the vector of predictor variables, $\\beta _{0k}$ and $\\beta _k$ are the intercept and coefficient vectors for category $k$, and $e$ is the base of the natural logarithm.\n",
    "\n",
    "The softmax function ensures that the probabilities for each category sum to 1. The numerator of the function represents the probability of category $k$, and the denominator represents the sum of the probabilities for all categories.\n",
    "\n",
    "We can estimate the coefficients using maximum likelihood estimation. The likelihood function for multinomial logistic regression is:\n",
    "\n",
    "$$L(\\beta) = \\prod _{i=1}^n \\prod _{k=1}^K P(Y_i=k|X_i=x_i)^{I(Y_i=k)}$$\n",
    "\n",
    "where $n$ is the number of observations, $I(Y_i=k)$ is an indicator function that equals 1 if $Y_i=k$ and 0 otherwise, and $P(Y_i=k|X_i=x_i)$ is the predicted probability of category $k$ for observation $i$.\n",
    "\n",
    "The negative log-likelihood function is:\n",
    "\n",
    "<a id='loss'></a>\n",
    "$$-l(\\beta) = -\\sum _{i=1}^n \\sum _{k=1}^K I(Y_i=k) \\log P(Y_i=k|X_i=x_i) \\tag{1}$$\n",
    "\n",
    "This is the function that we want to minimize in order to estimate the coefficients. We can use scope algorithm to find the values of $\\beta$ with sparsity constraints that minimize the negative log-likelihood function.\n",
    "\n",
    "Here is Python code for solving sparse gamma regression problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c91f44",
   "metadata": {},
   "source": [
    "### Data for multinomial logistic regression\n",
    "\n",
    "We import necessary packages and set a seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81c0c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from skscope import ScopeSolver\n",
    "import numpy as np\n",
    "from abess.datasets import make_multivariate_glm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bebb83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f49d60",
   "metadata": {},
   "source": [
    "\n",
    "Firstly, we shall conduct Multinomial logistic regression on an artificial dataset for demonstration. The `make_multivariate_glm_data` from `abess.datasets` function allows us to generate simulated data by specifying the `family=\"multinomial\"`. \n",
    "\n",
    "The assumption behind this model is that the response vector follows a multinomial distribution. The artificial dataset contains 500 observations and 20 predictors but only five predictors have influence on the three possible classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad40fc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real variables' index:\n",
      " {0, 3, 7, 10, 19}\n",
      "real variables:\n",
      " [[  5.44916029  -0.94953634   0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [ -1.39241163 -12.96678673   0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  3.24543565   4.02033588   0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [ -1.38210809   4.07755579   0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  5.79719104   3.61096451   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "n = 500  # sample size\n",
    "p = 20  # all predictors\n",
    "k = 5   # real predictors\n",
    "m = 3   # number of classes\n",
    "\n",
    "data = make_multivariate_glm_data(n=n, p=p, k=k, family=\"multinomial\", M=m)\n",
    "\n",
    "X = data.x\n",
    "y = data.y\n",
    "\n",
    "print('real variables\\' index:\\n', set(np.nonzero(data.coef_)[0]))\n",
    "print('real variables:\\n', data.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20d462",
   "metadata": {},
   "source": [
    "### Solve the problem\n",
    "\n",
    "Secondly, to carry out sparse-constrained optimization for multinomial logistic regression, we define the loss function `multinomial_regression_loss` accorting to [1](#loss) that matches the data generating function `make_multivariate_glm_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4b7d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomial_regression_loss(params):\n",
    "    beta = params.reshape((p, m))\n",
    "    # Compute the logits\n",
    "    logits = jnp.dot(X, beta)\n",
    "\n",
    "    # Compute the softmax probabilities\n",
    "    softmax_probs = jnp.exp(logits) / jnp.sum(jnp.exp(logits), axis=1, keepdims=True)\n",
    "\n",
    "    # Compute the NLL loss\n",
    "    loss = -jnp.mean(jnp.sum(y * jnp.log(softmax_probs), axis=1))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d00a115",
   "metadata": {},
   "source": [
    "We use `skscope` to solve the sparse multinomial logistic regression problem.\n",
    "After defining the data and the loss function, we can call `ScopeSolver` to solve the sparse-constrained optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aa62051",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = ScopeSolver(p*(m), k, group=[i for i in range(p) for j in range(m)])\n",
    "params = solver.solve(multinomial_regression_loss, jit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c74cee2",
   "metadata": {},
   "source": [
    "Now the `solver.params` contains the coefficients of multinomial logistic model with no more than 5 variables. That is, those variables with a coefficient 0 is unused in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec38527e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.39822362 -2.50996204 -1.88825765]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 3.99429591 -9.68531333  5.69101694]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.93656944  2.00800227 -2.94457135]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-2.31647311  3.43094489 -1.11447062]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 2.54118658  0.82594677 -3.36713346]]\n"
     ]
    }
   ],
   "source": [
    "print(solver.params.reshape((p, m)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5770f5b5",
   "metadata": {},
   "source": [
    "We can further compare the coefficients estimated by `skscope` and the real coefficients in two-fold:\n",
    "\n",
    "* The true support set and the estimated support set\n",
    "\n",
    "* The true nonzero parameters and the estimated nonzero parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c6fcb86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real variables' index:\n",
      " {0, 3, 7, 10, 19}\n",
      "Estimated variables' index:\n",
      " {0, 3, 7, 10, 19}\n"
     ]
    }
   ],
   "source": [
    "print('real variables\\' index:\\n', set(np.nonzero(data.coef_)[0]))\n",
    "print('Estimated variables\\' index:\\n', set(np.nonzero(solver.params.reshape((p, m)))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3321d97b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True parameter:\n",
      " [[  5.44916029  -0.94953634   0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [ -1.39241163 -12.96678673   0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  3.24543565   4.02033588   0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [ -1.38210809   4.07755579   0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  5.79719104   3.61096451   0.        ]]\n",
      "Estimated parameter:\n",
      " [[ 4.39822362 -2.50996204 -1.88825765]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 3.99429591 -9.68531333  5.69101694]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.93656944  2.00800227 -2.94457135]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-2.31647311  3.43094489 -1.11447062]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 2.54118658  0.82594677 -3.36713346]]\n"
     ]
    }
   ],
   "source": [
    "print(\"True parameter:\\n\", data.coef_)\n",
    "print(\"Estimated parameter:\\n\", solver.params.reshape((p, m)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p38",
   "language": "python",
   "name": "p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
