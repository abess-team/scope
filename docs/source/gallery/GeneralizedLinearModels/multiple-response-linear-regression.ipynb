{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple response linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to use an example to show how the sparse-constrained optimization for multiple response linear regression works in our program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Multiple response linear regression, also known as multivariate linear regression, is an extension of the classical linear regression model that allows for multiple dependent variables. Instead of modeling a single response variable, this approach models multiple response variables simultaneously, capturing the relationships between them and the set of predictor variables. This method is particularly useful when the response variables are correlated, as it accounts for the potential dependencies between them.\n",
    "Here are some examples of its practical applications:\n",
    "\n",
    "- **Environmental science**: In environmental science, multiple response linear regression is used to model the relationship between various environmental factors and multiple indicators of ecosystem health. \n",
    "- **Finance and economics**: Economists might use this method to model how interest rates, inflation, and unemployment rates jointly affect GDP growth, stock market returns, and consumer spending.\n",
    "- **Engineering**: In engineering, multiple response linear regression is applied in quality control and process optimization. Engineers might use this method to model how different manufacturing process parameters affect multiple quality metrics of a product, such as strength, durability, and efficiency. \n",
    "\n",
    "\n",
    "The model is expressed as:\n",
    "$$\n",
    "    y = B^* x + \\epsilon,\n",
    "$$\n",
    "where $y$ is an $m$-dimensional response variable, $x$ is $p$-dimensional predictors, $B \\in R^{m \\times p}$ is the sparse coefficient matrix, $\\epsilon$ is an $m$-dimensional random noise variable with zero mean.\n",
    "\n",
    "With $n$ independent data of the explanatory variables $X$ and the response variable $Y$, we can estimate $B^* $ by minimizing the objective function under sparsity constraint:\n",
    "\n",
    "<a id='loss'></a>\n",
    "$$ arg\\min_{B}L(B) := ||Y-B X||^2, s.t.  || B ||_ {0,2} \\leq s, \\tag{1}$$\n",
    "where $|| B ||_ {0, 2}$ is the number of non-zero rows of $B$.\n",
    "\n",
    "Here is Python code for solving sparse multiple linear regression problem:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for multiple response linear regression\n",
    "\n",
    "We import necessary packages and set a seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skscope import ScopeSolver\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Firstly, we shall conduct multiple response linear regression on an artificial dataset for demonstration. The `make_regression` from `sklearn.datasets` function allows us to generate simulated data. The artificial dataset contains 500 observations and 20 predictors but only five predictors have influence on the three possible responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real variables' index:\n",
      " {2, 9, 15, 16, 17}\n",
      "real variables:\n",
      " [[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [35.47424401  1.60554839 50.64398325]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [90.2768738  50.5206623  97.04288743]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [91.25666229 94.36559002 41.8819058 ]\n",
      " [63.67968592 92.21002028 18.0026354 ]\n",
      " [17.80823712 59.56585099 46.58491993]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "n, p, k, m = 500, 20, 5, 3\n",
    "x, y, coef = make_regression(n_samples=n, n_features=p, n_informative=k, n_targets=m, coef=True)\n",
    "\n",
    "print('real variables\\' index:\\n', set(np.nonzero(coef)[0]))\n",
    "print('real variables:\\n', coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve the problem\n",
    "\n",
    "Secondly, to carry out sparse-constrained optimization for multiple response linear regression loss, we define the loss function `multi_linear_objective` accorting to [1](#loss) that matches the data generating function `make_regression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_linear_objective(params):\n",
    "    return jnp.sum(jnp.square(y - jnp.matmul(x, params.reshape((p, m)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `skscope` to solve the sparse multiple response linear regression problem\n",
    "After defining the data and the loss function, we can call `ScopeSolver` to solve the sparse-constrained optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = ScopeSolver(p * m, k, group=[i for i in range(p) for j in range(m)])\n",
    "params = solver.solve(multi_linear_objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the `solver.params` contains the coefficients of multiple response linear regression model with no more than 5 variables. That is, those variables with a coefficient 0 is unused in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [35.4742443   1.60554861 50.64398376]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [90.27687424 50.52066226 97.04288718]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [91.25666144 94.36558946 41.88190469]\n",
      " [63.67968542 92.21001962 18.00263606]\n",
      " [17.80823746 59.56585047 46.58492027]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(solver.params.reshape((p, m)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further compare the coefficients estimated by `skscope` and the real coefficients in two-fold:\n",
    "\n",
    "* The true support set and the estimated support set\n",
    "\n",
    "* The true nonzero parameters and the estimated nonzero parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real variables' index:\n",
      " {2, 9, 15, 16, 17}\n",
      "Estimated variables' index:\n",
      " {2, 9, 15, 16, 17}\n"
     ]
    }
   ],
   "source": [
    "print('real variables\\' index:\\n', set(np.nonzero(coef)[0]))\n",
    "print('Estimated variables\\' index:\\n', set(np.nonzero(solver.params.reshape((p, m)))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated parameter:\n",
      " [[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [35.4742443   1.60554861 50.64398376]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [90.27687424 50.52066226 97.04288718]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [91.25666144 94.36558946 41.88190469]\n",
      " [63.67968542 92.21001962 18.00263606]\n",
      " [17.80823746 59.56585047 46.58492027]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "True parameter:\n",
      " [[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [35.47424401  1.60554839 50.64398325]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [90.2768738  50.5206623  97.04288743]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [91.25666229 94.36559002 41.8819058 ]\n",
      " [63.67968592 92.21002028 18.0026354 ]\n",
      " [17.80823712 59.56585099 46.58491993]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Estimated parameter:\\n\", solver.params.reshape((p, m)))\n",
    "print(\"True parameter:\\n\", coef)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p38",
   "language": "python",
   "name": "p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b3267657c2ca56fed6d7fdb7edada955d98b24e50acd65b198d3fe30cfd65ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
